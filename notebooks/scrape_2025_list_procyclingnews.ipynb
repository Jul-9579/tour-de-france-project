{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the Final Master Startlist from ProCyclingStats (first July week)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_latest_startlist(year=2025):\n",
    "    \"\"\"\n",
    "    Scrapes the master startlist for a given year of the Tour de France.\n",
    "    This function gets only the team and rider names.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.procyclingstats.com/race/tour-de-france/{year}/startlist\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    print(f\"Fetching LATEST startlist from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    all_riders = []\n",
    "    team_list_container = soup.find('ul', class_='startlist_v4')\n",
    "    if not team_list_container:\n",
    "        print(\"Could not find startlist container. Website structure may have changed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    teams = team_list_container.find_all('li', recursive=False)\n",
    "    print(f\"Found {len(teams)} teams. Processing...\")\n",
    "\n",
    "    for team in teams:\n",
    "        team_name_tag = team.find('h4').find('a')\n",
    "        if not team_name_tag: continue\n",
    "        team_name = team_name_tag.text.strip()\n",
    "        rider_tags = team.find_all('a')\n",
    "        for rider_tag in rider_tags:\n",
    "            rider_name = rider_tag.text.strip()\n",
    "            # This logic ensures we only get rider names, not team names or other links\n",
    "            if \" \" in rider_name and rider_name != team_name and len(rider_name) > 2:\n",
    "                rider_url = rider_tag['href']\n",
    "                all_riders.append({\n",
    "                    \"team\": team_name,\n",
    "                    \"rider_name\": rider_name,\n",
    "                    \"rider_url\": \"https://www.procyclingstats.com/\" + rider_url\n",
    "                })\n",
    "    \n",
    "    if not all_riders:\n",
    "        print(\"No riders were scraped.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(all_riders)\n",
    "\n",
    "# --- Execute the scrape on Monday ---\n",
    "print(\"--- Step 1: Scraping latest official startlist ---\")\n",
    "latest_startlist_df = scrape_latest_startlist()\n",
    "\n",
    "if not latest_startlist_df.empty:\n",
    "    print(f\"✅ Success! Found {len(latest_startlist_df)} riders on the latest list.\")\n",
    "    display(latest_startlist_df.head())\n",
    "else:\n",
    "    print(\"❌ Scraping failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare, Enrich, and Create Final Dataset\n",
    "\n",
    "def scrape_rider_details(rider_url):\n",
    "    \"\"\"Scrapes the detailed information for a single rider from their profile page.\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(rider_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        details = {'rider_url': rider_url}\n",
    "        \n",
    "        # Extract birthdate, nationality, height, weight\n",
    "        info_div = soup.find('div', class_='rdr-info-cont')\n",
    "        if info_div:\n",
    "            # Using .get_text(strip=True) is more robust\n",
    "            details['birthdate'] = info_div.find(string=lambda t: '19' in t or '20' in t, recursive=True) or None\n",
    "            if details['birthdate']:\n",
    "                details['birthdate'] = details['birthdate'].split('(')[0].strip()\n",
    "            \n",
    "            details['nationality'] = info_div.find('span', class_='flag').next_sibling.strip() or None\n",
    "            \n",
    "            height_tag = info_div.find(string=lambda t: 'm' in t and t.replace('.', '', 1).isdigit())\n",
    "            details['height'] = float(height_tag.replace('m', '')) if height_tag else None\n",
    "\n",
    "            weight_tag = info_div.find(string=lambda t: 'kg' in t)\n",
    "            details['weight'] = float(weight_tag.replace('kg', '')) if weight_tag else None\n",
    "\n",
    "        # Extract specialties\n",
    "        specialties_div = soup.find('div', class_='pps')\n",
    "        if specialties_div:\n",
    "            for specialty in specialties_div.find_all('div'):\n",
    "                name = specialty.find('div').text.lower().replace(' ', '_')\n",
    "                value = int(specialty.find('div', class_='pnt').text)\n",
    "                details[name] = value\n",
    "        \n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print(f\"  - Could not scrape details for {rider_url}. Error: {e}\")\n",
    "        return {'rider_url': rider_url} # Return URL to identify failure\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(\"\\n--- Step 2: Updating your detailed rider list ---\")\n",
    "\n",
    "# 1. Load your existing detailed dataset\n",
    "try:\n",
    "    existing_detailed_df = pd.read_csv(r'C:\\Users\\anonym\\Documents\\Bootcamp\\tour-de-france-project\\notebooks\\tdf_2025_startlist_full_details.csv')\n",
    "    print(f\"Loaded {len(existing_detailed_df)} riders from your existing file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find your existing detailed CSV file. Please check the path.\")\n",
    "    existing_detailed_df = pd.DataFrame(columns=['rider_name']) # Create empty df to avoid errors\n",
    "\n",
    "\n",
    "# 2. Identify riders to remove and add\n",
    "existing_names = set(existing_detailed_df['rider_name'])\n",
    "latest_names = set(latest_startlist_df['rider_name'])\n",
    "\n",
    "riders_to_remove = existing_names - latest_names\n",
    "riders_to_add = latest_names - existing_names\n",
    "\n",
    "print(f\"Riders to REMOVE: {len(riders_to_remove)}. Names: {list(riders_to_remove)}\")\n",
    "print(f\"Riders to ADD: {len(riders_to_add)}. Names: {list(riders_to_add)}\")\n",
    "\n",
    "\n",
    "# 3. Remove withdrawn riders\n",
    "df_after_removals = existing_detailed_df[~existing_detailed_df['rider_name'].isin(riders_to_remove)]\n",
    "\n",
    "\n",
    "# 4. Scrape details for new riders\n",
    "new_riders_data = []\n",
    "if riders_to_add:\n",
    "    print(\"\\nScraping details for new riders...\")\n",
    "    new_rider_info = latest_startlist_df[latest_startlist_df['rider_name'].isin(riders_to_add)]\n",
    "    \n",
    "    for index, rider in new_rider_info.iterrows():\n",
    "        print(f\" - Scraping {rider['rider_name']}...\")\n",
    "        details = scrape_rider_details(rider['rider_url'])\n",
    "        full_details = {**rider, **details} # Combine basic info with scraped details\n",
    "        new_riders_data.append(full_details)\n",
    "        time.sleep(random.uniform(1, 2)) # Be polite to the server\n",
    "    \n",
    "    new_riders_df = pd.DataFrame(new_riders_data)\n",
    "else:\n",
    "    new_riders_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# 5. Combine old and new data to create the final, updated DataFrame\n",
    "final_updated_df = pd.concat([df_after_removals, new_riders_df], ignore_index=True)\n",
    "\n",
    "# 6. Save the final, updated startlist for your pipeline\n",
    "output_filename = r'C:\\Users\\raclo\\Documents\\Bootcamp\\tour-de-france-project\\notebooks\\tdf_2025_startlist_full_details_FINAL.csv'\n",
    "final_updated_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"✅ FINAL STARTLIST CREATED! Saved to '{output_filename}'\")\n",
    "print(f\"Total riders in final list: {len(final_updated_df)}\")\n",
    "print(\"This is the file you will use as the input for your main prediction pipeline.\")\n",
    "print(\"=\"*50)\n",
    "display(final_updated_df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
